/////////////////////////// SISTEMAS CO-CREATIVOS
[1]A. Roberts, J. Engel, C. Raffel, C. Hawthorne, y D. Eck, “A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music”, arXiv:1803.05428 [cs, eess, stat], nov. 2019, Consultado: abr. 21, 2020. [En línea]. Disponible en: http://arxiv.org/abs/1803.05428.
...x
    .Genotype Eval: Reconstruction accuracy, NLL and probability dists on Latent RNN and VAE networks, RNN LSTM learning, 
    .Phenotype Eval: Participants were presented with two 16-bar (≈30s) compositions that were either sampled from one of the models or extracted from our evaluation dataset. They were then asked which they thought was more musical on a Likert scale.
    .Generation: decoder sampling of latent vectors corresponding to generated music
    .Domain: Music, melody generation and interpolation
    .Knowledge Base: The datasets were built by first searching the web for publicly-available MIDI files, resulting in ≈1.5 million unique files. We removed those that were identified as having a non-4/4 time signature and used the encoded tempo to determine bar boundaries, quantizing to 16 notes per bar (16th notes).
    .Aesthetic: The aesthetic is derived from the dataset, and it is representative of composing melodies similar as possible to the ones in the dataset literally by the accuracy of reconstructing it
    .Genotype: MIDI
    .Conceptualization: Deep learning model VAE LSTM RNN
    .Phenotype: Rendered Tracks
    .Collaboration: Division de tareas [define contraints (initial and ending contexts) , complete melody interpolating the inputs]
    -----x
    monophonic and Polyphonic
    VAE bi LSTM and RNN 
    minimum block is a beat
    composer with no possible realtime cappabilites
    inpainting sampling VAE SAMPLING?
    python tensorflow
    MIDI 
    The datasets were built by first searching the web for
    publicly-available MIDI files, resulting in ≈1.5 million
    unique files. We removed those that were identified as having a non-4/4 time signature and used the encoded tempo
    to determine bar boundaries, quantizing to 16 notes per bar
    (16th notes).
    "symbolic" data type
    opensource
...
[2]M. Hamanaka, K. Hirata, y S. Tojo, “Applying Melody-Morphing Method to Composition”, p. 16.
...x
    .Domain: Music, melody generation and interpolation
    .Knowledge Base: Rule defined construction procedure.
    .Aesthetic: The aesthetic is derived from the music thery revised, in this case GTTM
    .Genotype: MusicXML
    .Genotype Eval: Implicit in the time span tree construction if it lies within the boundaries defined by the rules of gneration 
    .Phenotype Eval: Musicologist reviewing the resulting generation 
    .Generation: decoder sampling of latent vectors corresponding to generated music
    .Conceptualization: applying the construction rules on the given melody input
    .Phenotype: Rendered Melodies
    .Collaboration: Division de tareas [define contraints (initial and ending contexts) , complete melody interpolating the inputs]
    -------x
    monophonic
    GTTM (Generative Theory of Tonal Music)  timespan trees
    minimum block is a beat
    composer with no possible realtime cappabilites
    morphing operations from one melody to other
    JAVA
    MusicXML 
    300 various COMPOSITIONS in musicxml format
    "symbolic" data type
    no public source code
...
[3]H. Lim, S. Rhyu, y K. Lee, “Chord Generation from Symbolic Melody Using BLSTM Networks”, dic. 2017, Consultado: abr. 21, 2020. [En línea]. Disponible en: https://arxiv.org/abs/1712.01011v1.
...x
    .Domain: Music, chord generation given a melody
    .Knowledge Base:  Wikifonia.org,  2,252 MusicXML lead sheets, which are all in major key
    .Aesthetic: The aesthetic is derived from the dataset, and it is representative of composing melodies similar as possible to the ones in the dataset accuracy of prediction and classification
    .Genotype: MusicXML
    .Genotype Eval: classification accuracy BLSTM metrics, 
    .Phenotype Eval: 25 musically untrained participants (13 males and 12 females) through a web-based survey. The participants complete 18 sets of surveys in their own pace. At the beginning of each set, participants listen to a melody. After that, participants listen to the four types of chord progressions, including the one from the original song, along with the melody. Participants are asked to rate each chord progression on a five-point scale (1 – ‘not appropriate’; 5 – ‘very appropriate’). At the end of each set, participants also are asked to answer a question whether they have pre-existing familiarity with the original songs.
    .Generation: Prediction of the BLSTM as inference for classification
    .Conceptualization: Deep learning model BLSTM
    .Phenotype: Rendered Melodies
    .Collaboration: Division de tareas [define melodia , compose chords]
    -----x
    monophonic
    bidirectional long short-term memory (BLSTM) networks
    minimum block is a beat
    composer with no possible realtime cappabilites
    Prediction of the BLSTM as inference for classification 
    *NO INFO if python or what*
    MusicXML 
    "symbolic" data type
    no opensource (there is a fanmade implementation though)
...
[4]T. Kitahara, S. Giraldo, y R. Ramírez, “JamSketch: Improvisation Support System with GA-Based Melody Creation from User’s Drawing”, en Music Technology with Swing, Cham, 2018, pp. 509–521, doi: 10.1007/978-3-030-01692-0_34.
...x
    .Domain: Music, melody generation given a drawing
    .Knowledge Base:  Weimar Jazz Database 456 scores stored as event descriptions in SQL
    .Aesthetic: The aesthetic is derived from the dataset, and it is representative of composing melodies similar as possible to the ones in the dataset accuracy of probability distributions
    .Genotype: SQL, MIDI
    .Genotype Eval: classification accuracy N-gram model, fitness function 
    .Phenotype Eval:  We asked 12 researchers and students in music-related fields to evaluate each performance from the following criteria on a scale of 0 to 10:
    .Generation: Evolution of the population of melodies in GA
    .Conceptualization: N-gram training definition of fitness function
    .Phenotype: Rendered Melodies
    .Collaboration: Turnos explícitos [dibuja contorno , compone melodía]
    -------x
    monophonic
    Genetic Algorithms
    minimum block is a beat
    composer with possible realtime cappabilites
    Evolution of a melody correspondingto the drawing 
    groovy language, java
    Weimar Jazz Database 456 scores stored as event descriptions in SQL 
    "symbolic" data type
    no opensource (there is a third-party implementation though)
...
[5]I. Simon, A. Roberts, C. Raffel, J. Engel, C. Hawthorne, y D. Eck, “Learning a Latent Space of Multitrack Measures”, arXiv:1806.00195 [cs, eess, stat], jun. 2018, Consultado: abr. 21, 2020. [En línea]. Disponible en: http://arxiv.org/abs/1806.00195.
...x
musicVAE but with polyphonic tracks
...
[6]J. Gillick, A. Roberts, J. Engel, D. Eck, y D. Bamman, “Learning to Groove with Inverse Sequence Transformations”, may 2019, Consultado: jun. 06, 2020. [En línea]. Disponible en: https://arxiv.org/abs/1905.06118v2.
...x
    monophonic
    VAE LSTMS
    minimum block is a beat
    composer with no possible realtime cappabilites
    Prediction of the BLSTM as inference for classification 
    python tensorflow
    MIDI
    "symbolic" data type
    opensource
    -------x
    .Domain: Music, rythm sequences to drums and infilling drumming
    .Knowledge Base: Groove MIDI Dataset (GMD), contains 13.6 hours, 1,150 MIDI files, and over 22,000 measures of tempo-aligned expressive drumming,
    .Aesthetic: The aesthetic is derived from the dataset, and it is representative of composing melodies similar as possible to the ones in the dataset accuracy of prediction and classification
    .Genotype: MIDI
    .Genotype Eval: classification accuracy BLSTM metrics, 
    .Phenotype Eval: We presented participants with random pairs of clips, one of which was generated by each model, asking them to judge which clip sounds more like a human
    .Generation: decoder in VAE "sampling"
    .Conceptualization: Deep learning model VAE architecture 
    .Phenotype: Rendered MIDI
    .Collaboration: turnos explícitos/division de tareas (las tareas son excluyentes un agente hace cada una pero tomando en cuenta el resultado del otro) [define ritmo/ basic drumming, compose basic drumming or infill]
...
[7]Tetsurou KITAHARA, "Generating Walking Bass Lines with HMM”. https://researchmap.jp/read0151518/published_papers/18183487 (consultado jun. 06, 2020).
...x
    .Domain: Music, walking bass line given chord progression
    .Knowledge Base: data collected from “Jazz Bass Learning: 104 Examples Collections 1–3” [11–13] and the website Projazz Lab [14]
    .Aesthetic: The aesthetic is derived from the dataset, and it is representative of composing melodies similar as possible to the ones in the dataset accuracy of prediction and classification and 9 specific criterion
    .Genotype: MIDI
    .Genotype Eval: training accuracy HMM metrics and 9 specific quantitative metrics, 
    .Phenotype Eval: We asked an expert bassist with 25 years of experience in playing jazz bass to evaluate the bass lines generated by the three methods
    .Generation: estimated most likely notes in HMM
    .Conceptualization: HMM learning
    .Phenotype: Rendered MIDI
    .Collaboration: turnos explícitos/division de tareas (las tareas son excluyentes un agente hace cada una pero tomando en cuenta el resultado del otro) [define chord progressions, compose walking bass line]
    -------------x
    monophonic
    HMM
    minimum block is a beat
    composer with no possible realtime cappabilites
    Prediction of the HMM
    *NOT SAYS ANYTHING ABOUT ITS IMPLEMENTATION*
    data collected from “Jazz Bass Learning: 104 Examples Collections 1–3” [11–13] and the website Projazz Lab [14]
    "symbolic" data type
    NO opensource
...
[8]“MorpheuS: Generating Structured Music with Constrained Patterns and Tension - IEEE Journals & Magazine”. https://ieeexplore.ieee.org/document/8007229 (consultado jun. 10, 2020).
...x
    .Domain: Music, Writting polyphonic tracks according to a template or tension profile
    .Knowledge Base: rule based on tension profiles infused in the optimization algorithm
    .Aesthetic: The aesthetic is derived from the rule based on tension profiles infused in the optimization algorithm and similarity on the tension and template
    .Genotype: MIDI
    .Genotype Eval: Optmiziation function metrics
    .Phenotype Eval: Several expert listeners remarked on the humor apparent in MorpheuS’ pieces, particularly the ways in which it naively violates conventional expectations, often with surprising and sometimes adventurous but plausible results. The advantage of this naivete, as one person puts it, is ´ that the system “has no fear” and thus “has courage to try things”
    .Generation: Population evolution starting from random
    .Conceptualization: Definition on the tension and rule based optimization algorithm
    .Phenotype: Rendered MIDI
    .Collaboration: turnos explícitos [define tension profile or give a template, compose tracks]
    -----x
    polyphonic
    Pattern recognition and optimization algorithms  (There has been research on pattern detection techniques for music audio [77, 78], but our focus is on symbolic music (i.e. MIDI). MorpheuS uses two state-of-the-art greedy compressionbased algorithms for MIDI, COSIATEC and SIATECCompress [9], both based on Meredith’s “Structure Induction Algorithm” (SIA) and SIATEC. SIA finds all the maximal translatable patterns (MTP) in a point-set and SIATEC discovers all translational equivalence classes (TECs) of MTPs in a point-se /// FOR Optmiziation > variable neighborhood search)
    minimum block is a beat
    composer with no possible realtime cappabilites
    ITerations over the optimization algorithm
    JAVA
    rule based on tension profiles infused in the optimization algorithm
    "symbolic" data type
    NO opensource 
...
[9]B. L. Sturm, J. F. Santos, O. Ben-Tal, y I. Korshunova, “Music transcription modelling and composition using deep learning”, arXiv:1604.08723 [cs], abr. 2016, Consultado: jun. 10, 2020. [En línea]. Disponible en: http://arxiv.org/abs/1604.08723.
...x
    .Domain: Music, Generation of continuation melodies
    .Knowledge Base: ddata collected from the session.org abc format 23,636 transcriptions scores
    .Aesthetic: The aesthetic is derived from the dataset, and it is representative of composing melodies similar as possible to the ones in the dataset accuracy of prediction and classification
    .Genotype: ABC format
    .Genotype Eval: classification accuracy LSTM metrics, statitical evaluations
    .Phenotype Eval: One user listened to several, and identified the example below, saying, “In the tune below, the first two phrases are quite fun as a generative idea to ‘humancompose’ the rest of it! I know that’s not quite the point of course. Still had
    .Generation: LSTM sampling 
    .Conceptualization: RNN LSTM learning
    .Phenotype: Rendered MIDI, (or is it?)
    .Collaboration: turnos explícitos [define seed initial part, compose the rest]
    ------x
    monophonic
    RNN LSTM
    minimum block is a beat
    composer with no possible realtime cappabilites
    Prediction, sampling from the RNN
    python theano
    data collected from the session.org abc format 23,636 transcriptions scores
    ABC "symbolic" data type
    opensource    
...
[10]A. R. Brown, “Creative improvisation with a reflexive musical bot”, Digital Creativity, vol. 29, núm. 1, pp. 5–18, ene. 2018, doi: 10.1080/14626268.2017.1419979.
...
    .Domain: Music, Generation of duet accompainment
    .Knowledge Base: The CIM system includes some cultural ‘knowledge’, such as regularity of tempo and constraints around the density of material to perform, but it does not contain a database of learned phrases or structures.
    .Aesthetic: AND PERSINAL USSER TASTE BY HIS PERFORMANCE AS DUET The CIM system includes some cultural ‘knowledge’, such as regularity of tempo and constraints around the density of material to perform, but it does not contain a database of learned phrases or structures.
    .Genotype: ABC format, directly rendered
    .GENOTYPE AND PHENOTYPE SAME EVAL: There have been several previous experiments and performances with different versions of CIM (Brown, Gifford, and Voltz 2013a). The results reported here are based on recent trials in which six expert improvising musicians used the latest version of CIM. Four of the six had some previous experience with CIM—in two cases quite substantial—and the remaining two had none. The musicians explored the system by rehearsing with it for several hours, with guidance from researchers
    .Generation: REflexive and random sampling from database of current audio recorded from the user
    .Conceptualization: Rule definition and user recorded material for sampling
    .Collaboration: Iniciativa mixta
    -----x
    monophonic
    REflexive and random sampling from dataset
    minimum block is a beat
    composer with realtime cappabilites
    REflexive and random sampling from dataset
    *NO IMPLEMENTATION DETAILS*
    The CIM system includes some cultural ‘knowledge’, such as regularity of tempo
        and constraints around the density of material
        to perform, but it does not contain a database
        of learned phrases or structures.
    MIDI
    NO opensource
...
/////////////////////////// METODOLOGIAS
